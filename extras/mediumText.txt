ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 30, 2020Member-onlySaveOpenAI’s GPT-3 is a powerful text-generating neural network pre-trained on the largest corpus of text to date, capable of uncanny predictive text response based on its input, and is currently by far the most powerful language model built.GPT is an acronym for Generative Pre-Trained Transformer. GPT-2, announced in February 2019 by OpenAI, was trained on the WebText dataset which contained over 8 million documents or 38GB of text data extracted from Reddit submissions. In November 2019, the final version of the GPT-2 was released, containing pre-training on 1.5 billion parameters.To put it in perspective, GPT-3 has been trained on 175 billion parameters and under a trillion words, making its predecessor GPT-2 with 1.5 billion parameters look miniature at one hundredth the size. And note that GPT-2 was officially released less than a year earlier in November 2019. The next biggest model is Google’s T5 which comes in at only 11 billion parameters.GPT-3’s capabilities at predicting text and language are uncanny. It’s able to write functioning code, can respond with human sounding dialogue, generate images, write articles, fictional stories, books, or even a mundane task of writing an email.The predictions are not always perfect, for one GPT-3 doesn’t actually understand what the words mean. Read — Any Limitations?At its simplest, GPT-3 takes a phrase of input text and predicts what the next text output should be. This type of machine learning doesn’t “think”, it processes the textual input based on it’s previously trained data and runtime translators.Pre-training happens on a massive dataset, including the public internet, book corpus, and Wikipedia. By vastly widening the examples to train on, it improves the quality and performance of its response. Being so large, GPT-3 was estimated to cost approximately a whopping $5 million USD to train, which brings into question cost scalability for the future, next versions of GPT-3.One of the primary training datasets used to train GPT-3 came from CommonCrawl, which is a freely available public dataset consisting of crawls of the public web containing nearly a trillion words. CommonCrawl represented 60% of the weight in training and input more than 400 billion tokens.Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions — something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.Source arvix 2005.14165v4.Yes, the creators of GPT-3 recognize limitations. In the area of text synthesis:On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs.In comparison, human beings are capable of holding a persistent mental point of view, whereas GPT-3 can lose focus and “forget” over the course of longer passages.Within discrete language tasks, such as “common sense physics”:Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. Specifically GPT-3 has difficulty with questions of the type“If I put cheese into the fridge, will it melt?”.Not clear from this passage if “common sense physics” could be mitigated in the future by training on physics dataset(s).And this common and important concern with bias in most deep learning systems:Finally, GPT-3 shares some limitations common to most deep learning systems — its decisions are not easily interpretable,it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on.GPT-3 is a significant leap forward, but more in terms of scale, being it shares much of the same architecture of GPT-2. What it’s really about, is testing the hypothesis of scaling of language models for massively improved performance.Some of the more promising areas are GPT-3 as an augmented creativity tool, writing in general, code generation, or code co-pilot. This is not AI that will take over the world but should be remembered as a significant moment.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 25, 2020Member-onlyGPT-3 is a product developed by Elon Musk backed research lab OpenAI based in San Francisco. It is capable of generating code to develop web apps, generating text in the form of articles, poems, and…Jul 27, 2020Member-onlyCinematography and editing goes hand in hand. The art of cuts during the editing process is specially fascinating. It becomes clear when it should and shouldn’t be used, when to split up footage or the right camera positioning. This is also a key element in the flow of the storyline. Take my favourite scenes with brutal killings. There are two different ways to film it. One is to have a wide shot and the other is to focus on the face. That’s also why you see some scenes coming back to. In the first case the person is in the centre of the screen and in the second case it will be…Jul 27, 2020Member-onlySaveGPT-3 has taken the world by storm. There are thousands of tweets about it with numerous mind-blowing use cases which you can see for yourself if you search Twitter for #gpt3 hashtag.In this text, I have taken a bunch of them to show general trends. In brief, GPT-3 allows humans to communicate with machines in Simple English. That means that by simply describing what you want to do you can get:GPT-3 definitely will influence how we communicate with our devices and lower the level of technical sophistication one needs to build new applications. As such it will be a tool in a process of democratization of AI.If you want to test similar models yourself, check out Contentyze.If you want to test similar models yourself, check out Contentyze. This is the platform I’m building with my team, with a goal to make creating texts much simpler. Our beta version is now open for tests.Finally if you prefer a video version of this text, have a look here:Also in my previous video I have analysed the actual paper from OpenAI on GPT3. So if you have a technical background and what to learn more about GPT-3 in the context of Transformers and past models like Megatron, Turing-NLG and GPT-2 have a look at this video:If you want to stay tuned for more news about AI, technology, and machine learning, subscribe to my newsletter here:ERROR: Could not find article textJul 30, 2020Member-onlyImagine, you let two Artificial Intelligence Agents alone in one room. What may happen? I was curious and used GPT-3 playground for such, let say, voyeuresque stage. Well, that escalated quickly…For my experiment, I used following setting:ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 28, 2020Member-onlyHow GPT-3, your smartphone and Augmented Reality can disrupt a dinosaur industry.We all love a good picture. The history of photographic studios and photography dates back to 19th century with the first camera. The earliest photographic studios made use of painters’ lighting techniques to create portraits. In my country, generations of Indians would assemble under the studio lights to get that perfect family portrait. We have come a staggering distance since then.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 30, 2020Member-onlyUnless you’ve been living under a rock the past few weeks, I bet you’ve seen a lot of discussions about GPT-3, OpenAI’s newest language model. Think of GPT-3 as an extremely sophisticated text predictor — it has effectively ingested all the information on the internet to provide you with statistically plausible answers. I can…ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJun 16, 2020Member-onlyIs it a Memory Bank or an NLP Ultimatum?So last week OpenAI told the world about this gigantic model which was trained on 174,600 million parameters and released a 74-page research paper titled Language Models are Few-Shot Learners.If you don’t get the depth of this number then see the parameters used in the past models.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 26, 2020Member-onlyIf you hang out on the fringes of the Internet you may have seen very smart people losing their minds over something called GPT-3.GPT-3 is an artificially intelligent platform that uses information scraped from the…ERROR: Could not find article textERROR: Could not find article textJul 17, 2020Member-onlyBERT has been open sourced on GitHub, and also uploaded to TF Hub.I think the best way to understand it is to play with its code. The README file on GitHub provides a great description on what it is and how it works:BERT — Bidirectional Encoder Representations from Transformers is a method of pre-training language representations, meaning that we train a general-purpose “language understanding” model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms…ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 24, 2020Member-onlySaveLike Sam’s tweets, there is a lot of hype in the tech community about the latest GPT-3 released by OpenAI in June 2020, but it is still powerful and impressive when you interact with it. GPT-3 is the largest language model ever trained and achieved good results on several NLP tasks like language generation and language translation, with huge potentials for many other creative and functional tasks.Here we will go over a couple of highlights and have a more clear view of what the model can and cannot do, and how to utilize it to empower various applications with examples.GPT-3 use the Transformer framework and Attention architecture, which is the same model architecture as GPT-2. It comes with different sizes, the largest (or “GPT-3”) has 175B trainable parameters, 96 layers, 96 heads in each layer, each head with a dimension of128. Even the batch size is huge at 3.2M.It is trained on huge text corpus from sources likeAccording to the paper, to train the largest GPT-3(175B parameters) needs 3640 petaflop/s-day, and 1 petaflop/s-day is equivalent to 8 V100 GPUs at full efficiency of a day.“The supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server,” check out how Microsoft built the GPU-accelerated supercomputer system for OpenAIThis is the main breakthrough of the GPT-3, it means the model can quickly learn a task by observing zero, one, or few examples of the task, makes it more capable of predicting for tasks that it has never seen. The bigger model can learn the task better with contextual information from few examples.When comparing with traditional task agnostic architecture (BERT), which requires tens of thousands task specific examples to fine tune on a downstream task. GPT-3 enables us to perform on a new task (classification, question answering, translation, reasoning and many others) from just a few example or instructions without fine tuning. This is a huge advantage over other mainstream language models like BERT, ELMO, XLM.www.datadriveninvestor.comEven more, OpenAI released the GPT-3 API. Sending the text prompt, the API will return the completion, attempting to match the pattern you gave it in the examples. The API is so simple that developers can just plug and play to bring the intelligence of remote model to their products, without worry about ML pipeline, infrastructure, and hosting. This is a huge breakthrough with many benefits:The API is not ready for mass production use, OpenAI and developers are still learning and evaluating various applications and the potential social impact.Many creative prototypes enabled by GPT-3 are popping up in the past few weeks, a lot of impressive demos can be found on Twitter. Now, let’s have a deeper dive of how GPT-3 helps with traditional NLP tasks as well as some of the creative applications from community.** In the following examples, we will use blue 🔵 color to indicate the context or prompt (task descriptions, examples) as a prefix. Then the orange 🍊 color to indicate the users’s input and the rest will be the model’s predictions.One of the most impressive use cases is language generation/text completion. Here is an example of having GPT-3 to help us expand one sentence to create a short essay. It even provides concrete examples of how to express gratitude.With just 5 QA examples, the model is able to answer all the following questions with accurate answers while maintaining the context information.With description of the task and one example, the model is able to carry on a helpful, creative, and relevant chat experience. But in this example, the bot made up the response of Sam Altman is a co-founder of Y Combinator, while in reality he is the president. This is probably because I used a higher temperature (one of the api parameter) for higher creativeness.For this task, we provided descriptions of the task and 3 examples of company name, business category, and their latest market cap. Then we tested by providing 4 new names like Unilever, McDonald’s, Google, and Apple.The model finished the business category classification most accurately, and also able to estimate the market cap numbers. But due to the model is trained with data cut off in Oct 2019, so I guess the predicted market cap is outdated.With just 5 English to Chinese translation examples, the model did a pretty good job on finishing the rest of the translations.This example is to demonstrate the model’s text to action capabilities. A comparable use cases like Alexa or Siri where user give voice queries and we need to extract the semantic meanings of the query by parsing each query into an intent and multiple slots for actions.The following example presents you a text to actions NLU engine with few shots. It is amazing that with just a couple of examples, the model is able to help create a new intent “open-app” and extract all the slots correctly for “open Netflix and search house of cards”.Beyond traditional NLP tasks, there are many other amazing things we can build with this API.Like create fiction, writing songs, generating movie scripts, generating stories ends up to different directions, writing emails, help you generate ideas, telling kids a story, or even generate images …The following is a collection of some creative prototypes from the community, you will find huge potentials in different domains and modalities.GPT-3 creates a paradigm shift on developing AI productsTraditionally, as a ML product manager or ML engineer, you will spend more efforts on curating the training dataset, selecting the models, fine tuning, and running the evaluation. ML infra needs to orchestrate the hosting of multiple models for inference and scaling.With GPT-3 API, some of the above efforts can be skipped. Once you figured out what problem GPT-3 API can help solve, how you design the prompt and how you choose API parameters (temperature, response length and etc.) will impact the completion quality and success of the product. And product owners and developers also need to dive deep to explore other possible sources and modalities of the prompts, like user’s likes, clicks, sharing, and comments, even the transcript from the audio or semantic segmentations from the image/video.Social impactsGPT-3 is a good start, though there are some concerns on fairness, bias and social impacts. The more time users spend on using the products, the more it’ll affect the quality of the dataset. This will lead to potential bias of AI products from different providers, which are based on the same GPT-3 models.Some ideas to tackle the issues above:Create a trustable reputation system for ML models.Provide the tracking data to the models to improve the quality of answers. The users can have control on which provider to select, based on their need and quality.The efforts in training the models and evaluating the AI products are still critical. But GPT-3 still provides a new way of development AI products, with potential benefits on time, cost and quality.GPT-3 is powerful and super-intelligent. Now it is time to turn on our imagination to find all the possibilities.Feel free to suggest ideas or say hi.At the beginning I used “Demystify GPT3 with real use cases” as the headline. Before publishing this article, I feel it is so plain, so I asked GPT-3 for help. 😜The headline and several paragraph of this article are generated by GPT-3.ERROR: Could not find article textJul 21, 2020Member-onlyYeah. I ran the OpenAI.com — GPT-3 Research Paper through the GPT-3-Powered translator for 2nd graders in the OpenAI Example Documentation. You’re welcome?ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 29, 2020Member-onlyThe launch of Open AI’s 3rd generation of the pre-trained language model, GPT-3 (Generative Pre-trained Transformer) has got the data science fraternity buzzing with excitement!The world of Language Models (LM) is quite fascinating. To give a brief introduction, these models learn the probabilities of a sequence of words that occur in a commonly spoken…ERROR: Could not find article textERROR: Could not find article textJul 31, 2020Member-onlyEarlier this month AI think tank OpenAI released a closed beta version of their Language Modeling platform known as GPT3.GPT3 is the world’s largest language model by an order of magnitude, essentially it has been trained on the world’s websites using CommonCrawl.org, Wikipedia, and other well-known text corpa, its predecessor GPT2 was one of the world’s most advanced language models trained on 1.7 billion…ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJun 19, 2020Member-onlyOpenAI recently published a paper describing GPT-3, a deep-learning model for Natural Language Processing, with 175 Billion parameters(!!!), 100x more than the previous version, GPT-2. The model is pre-trained on nearly half a trillion words and achieves SOTA performance on several NLP benchmarks without fine-tuning.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 20, 2020Member-onlySaveGPT-3 is a language model created by OpenAI which can form natural language, given a prompt. Behind the scenes is a neural network with 175 billion parameters being fed massive amounts of data scraped from the internet. The result is a machine that can understand and respond to questions posed in GPT-3. It is not yet able to speak or write anything other than English. But it does have an understanding of basic grammar. This means that if you ask it to tell you what a triangle is, it will say ‘a three-sided object’ or something similar.I must admit: I hate writing introductions. So I wrote the first two sentences of the paragraph above, plugged it into AI Dungeon with some context, and out came the remainder of the paragraph. (As we’ll see in a moment, GPT-3 is severely underselling itself here.)It’s obviously not perfect, but some allowance should be made for the fact that AI Dungeon runs a weaker version of GPT-3 and is configured to be used as a text-based game rather than a way of cheating at writing. As the following sample demonstrates, the full GPT-3 (to which access is currently severely limited) could be mistaken for a college-level writer.There is an ongoing debate in my family about the origin of the word “cowboy.” We know the term was originally used to describe a herder or drover of cattle. Some of my family maintains it was coined in Spain, describing the Mexican or Spanish version of those workers.My father’s family, on the other hand, insists the term was used by Anglo cowboys to describe the tough but generally peaceful herders or drovers.The debate came up again a few weeks ago when we went to watch “Lone Ranger” with my father and his wife, my stepmother, Barb. The man riding the white horse was a sheriff, not a cowboy, they insisted.I finally admitted I didn’t really care where the term came from and we all had a good laugh over it. (from the GPT-3 repo; direct link)Occasionally, the AI’s awkwardness and repetition betray its identity:Glacier Ridge has had its students participate in various competitions and competitions. (from the GPT-3 repo; direct link)In all fairness, many human writers suffer from similar flaws. Few humans, however, can write in Spanish, French, Hungarian and Portuguese.Remember Eugene Goostman? In 2014, the chatbot made international headlines for passing the Turing Test, convincing 30% of a panel that it was human. Eugene Goostman is a 13 year-old boy from Ukraine, replete with the slang, initialisms, and an indifferent tone. Whether the chatbot successfully passed the Turing Test is still a subject of controversy; imitating a hostile 13 year-old is almost certainly an easier feat than passing as a human being in intellectual conversation.Naturally, I wanted to see how GPT-3 would fare in the Turing Test, but I needed some standard by which to evaluate its responses. Thankfully, Turing provided a sample dialogue between an interrogator and computer which I tried to replicate.In order to prompt GPT-3 to respond to the same line of questioning as in Turing’s dialogue, I first assigned the interrogator the name ‘Paul’ and the computer ‘Jack’. My purpose in doing so was to avoid any confusion that the Interrogator and Computer were two individual characters in the dialogue. Then, I provided some context (italicized) about the ensuing discussion. There are some parallels between the sample dialogue and my coaxing of AI Dungeon, but the conversation largely falls apart halfway through.Whether a single dialogue is an indication of being able to pass the Turing Test is doubtful. However, if you consider Goostman as having passed the Turing Test, it appears as if GPT-3 can do the same.Through some manipulation of AI Dungeon, we can interview imitations of famous people from a variety of fields. Here’s a snippet from fake Larry Page on big data:There are two main approaches [to improving our use of big data]. The first is automation, or what I like to think of as the smart pill…The other approach is to analyze the data in a different way. For example, what are the stories behind all these numbers? Why did this person buy something? What’s their story? If we can understand all that, we’ll be able to target advertising so it really matters to people.There is also a part of the interview where Page starts talking about people wanting to kill him when he was six years old as a result of his sister being born, so consider the above excerpt with a salt-shaker in hand. You can read the full text of this ‘interview’ here.And some inspiration for your next article:The following is the headline of an article published in Towards Data Science, a Medium publication hosted at towardsdatascience.com: “A new tool for data scientists to analyze their own code.”You open up the article and read it. It’s written by one of your colleagues, who goes on to explain that he or she has created a program called “Data Analysis Toolkit”, which allows you to easily analyze your own code.I must admit that I am already in possession of such a toolkit. It’s called “scrolling aimlessly trying to figure out why my code is broken”. Very innovative, if you ask me — apparently GPT-3 has different ideas.My experiences with GPT-3 via AI Dungeon deeply impressed me, but they also revealed its limitations. Ultimately, GPT-3 is uncannily imitating what has been scraped from the Internet rather than displaying general intelligence. Even the full version has trouble with all but the most basic arithmetic, and plays a comparatively weak game of chess.Judging it by human standards, GPT-3 can be mind-numbingly stupid. That hasn’t stopped it from performing tasks previously thought to be the domain of highly-intelligent humans. This Twitter thread collates some of its achievements, which include writing fiction in a variety of styles, generating a working React app from a simple description of its function, and legal writing.I am fairly convinced at this point that people will lose their jobs because of advancements in language models like GPT-3. The only question in my mind is whether this will happen in the next five years or the next ten. Artificial General Intelligence may be far away from realization, but intelligence isn’t a prerequisite to disrupting society.For now, I’ll enjoy playing AI Dungeon and try not to consider that I might have to compete with it for a job in a few years. Who am I kidding? When that time comes, there won’t be any competition.Jul 14, 2020Member-onlySaveUpdated: 18th of November 2021: “access without waitlist”It was last year in February, as OpenAI published results on their training of unsupervised language model GPT-2. Trained in 40Gb texts (8 Mio websites) and was able to predict words in proximity. GPT-2, a transformer-based language applied to self-attention, allowed us to generated very convincing and coherent texts. The quality was that good, so the main model with 1.5 billion parameters wasn’t initially publicly accessible, to prevent uncontrolled fake news. Luckily, the complete model was later published and could be even used with Colab Notebooks.This year OpenAI strikes back with new language model GPT-3. With 175 billion parameters (read also: GPT-3 Paper).Unnecessary spoiler: it’s incredibly good.There are already some profound articles on TDS examining features and paper of GPT-3:towardsdatascience.comtowardsdatascience.comtowardsdatascience.comOpenAI is building an API, currently accessible via waiting list:beta.openai.comFortunately, I could get access and experiment with GPT-3 directly. Here are some of my initial outcomes.Update (18.11.2021): OpenAI’s API is now available with no waitlist.The AI Playground interface looks simple, but it bears the power within. For the first, here is a setting dialog, which lets you configure text length, temperature (from low/boring to standard to chaotic/creative), and other features.You also can define where the generated text has to start and to stop, these are some of the control functions that have a direct impact on textual results.The simple interface provides also some GPT-3 presets. The amazing thing about transformer-driven GPT-models is among others the ability to recognize a specific style, text character, or structure. In case you begin with lists, GPT-3 continues generating lists. In case your prompt has a Q&A structure, it will be kept coherently. If you ask for a poem, it writes a poem.You can do your own presets, or use the existing, which are:Chat.A typical setting for a chatbot. You ask - AI answers. It’s possible to change the “characters” or setting also. As you can see, the chat situation was accomplished perfectly (even if my, Human’s, third question was kind of unfair).To demonstrate the contextual impact, let’s change the AI character from “helpful” and “very friendly” to “brutal, stupid and very unfriendly”. You will see how the whole dialogue will be influenced:I think, we re-invented Marvin the Paranoid Android.Q&AThis preset consists of a clear dual structure: Question and Answer. You need some training before it starts to answer the question (and get the rules), but then it works perfectly. I asked some random questions from various areas and here you go:I’d say, perfect!Parsing unstructured dataThis one is fascinating and shows a good comprehension of the unstructured text — extracting structured data from the full text.Summarizing for a 2nd graderThis preset shows another level of comprehension — including rephrasing of difficult concepts and sentences in clear words.I tried Wittgenstein:The simple proverb can be paraphrased convincingly:Or look at this pretty well and clear transition of Sigmund Freud’s time distancing concept:As you see, compression of text and its coherent “translation” is one of the strengths of GPT-3.GPT-2 was already a great language model when it was about English. You could generate amazing texts, especially with 1.5 billion parameters. I used GPT-2 for a screenplay of this short movie — and its absurdity could be rather understood as a good tradition of David Lynch and Beckett:The dialogues were logical, even if spontaneous. But it was regarding English. If you’ve tried with inputs in other languages, you would face the barrier of understanding. GPT-2 tried to imitate languages, but you needed to fine-tune it on text corpus in a specific language to get good results.GPT-3 is different.Its processing in other languages is phenomenal.I tried German, Russian, and Japanese.German.It was rather my daughter, who tried to let GPT-3 write a fairy tale. She began with “Eine Katze mit Flügeln ging im Park spazieren” (“A cat with wings took a walk in a park”).The emerged story was astonishingly well written. With irony, vivid characters, and some leitmotifs. This is not just a collection of topoi or connected sentences. This is… a story!Russian.I trained once GPT-2 on Pushkin’s poetry and have got some interesting neologisms, but it was a grammar mess. Here I input some lines of Pushkin’s poem — and the result I’ve got was… interesting. It hadn’t rhymes, but stylistically intense power. It was not Pushkin style, though. But almost without any mistakes or weird grammar. And… it works as poetry (especially if you are ready to interpret it).Japanese.This was something special. I entered just a random sentence:今日は楽しい一日になりますように！と言いました。// Today was funny and entertaining day, I said.And the result was a small story about prayer, happiness, wisdom, and financial investment. In well written Japanese (neutral politeness form, like the input).It does mean: GPT-3 is ready for multilingual text processing.My first try was, of course, to write a Shakespearean sonnet. So the prompt was just:The result was this:Perfect iambic verse, great style, nice rhymes… If not one thing:The first two lines are actually from Alexander Pope, The Rape of the Lock. And here we have a reason to be cautious: GPT-3 produces unique and unrepeatable texts, but it can reuse the whole quotes of existing texts it was trained on.Re-examination of results is inevitable if you want to guarantee a singularity of a text.I wonder, if there are some possibilities for “Projection” like StyleGAN2 feature, just in opposite to StyleGAN2 (where it compares the image with latent space), in GPT-3 it would compare with the dataset it was trained on? To prevent accidental plagiarism.But the thing is: GPT-3 can write poems on demand, in particular styles.Here is another example:As I still hadn’t accessed, I asked a friend to let GPT-3 write an essay on Kurt Schwitters, a German artist, and Dadaist:The outcome is: GPT-3 has already a rich knowledge, which can be recollected. It is not always reliable (you have to fine-tune it to have a perfect meaning match), but it’s still very close to the discourse.Another mindblowing possibility is using GPT-3 is quite different cases than just text generation:You can get support by CSS:And calling it General Intelligence is already a thing:We are still at the beginning, but the experiments with GPT-3 made by the AI community show its power, potential, and impact. We just have to use it with reason and good intention. But that’s the human factor. Which is not always the best one.For more wonderful text experiments I highly recommend you to read Gwern:www.gwern.netJul 27, 2020Member-onlyThis blog post provides an explanation of GPT-3 [1]. The summary of the content is as follows.ERROR: Could not find article textERROR: Could not find article textJul 30, 2020Member-onlyThis is a brief overview of our work at ICLR 2021. Code is available here. The code is very easy to run on Google Colab from the example provided in the code. Try it out yourself for fun! :)Tl;dr: We provide a new text decoding algorithm that directly controls generated text statistics and hence more human-like texts.Surprise in (text) data is something like sugar in your tea, you just need the right amount for the perfect taste. Too little of sugar makes your tea bland or…ERROR: Could not find article textERROR: Could not find article textJul 27, 2020Member-onlySaveA lot of ink has been spilled (or pixels illuminated) about the wonders of GPT-3, OpenAI’s latest and greatest language model. In the words of Venturebeat:A team of more than 30 OpenAI researchers have released a paper about GPT-3, a language model capable of achieving state-of-the-art results on a set of benchmark and unique natural language processing tasks that range from language translation to generating news articles to answering SAT questions.And as tweeted by MIT:To be fair, some of the examples are amazing:But like most examples spat out by language models, almost all of these were hand-selected by humans after many runs. Because not-so-good results just wouldn’t make the news.Even bearing that in mind, I’m still blown away by what I’ve seen of GPT-3. But right now there’s no way I can get my hands on it for any practical purposes — it’s only available to a few researchers at the moment. What about us mere mortals? Other than playing AI Dungeon of course…For now, we can play with GPT-3’s older brother, GPT-2. A few weeks ago I wrote easy_text_generator, a tool to generate text of any length from language models in your browser. You can use GPT-2 or a few pre-trained or distilled versions of it (downloaded from huggingface) to generate different kinds of text, like movie scripts, Star Trek scripts, IMDB movie reviews, or just text in general.Or if you want something with zero install, you can generate text directly from huggingface’s site, just with limited length. For example, try out GPT-2 here.As I said, most examples of GPT-3 you may have seen have been cherry-picked. To make a reasonably fair comparison, I’ll run each language model 5 times and cherry-pick one output for each model. The other ones I’ll put in another post, otherwise, this blog post will get too long.Let’s look at general purpose text at first. Since I just started working at a company doing a neural search, we’ll use some appropriate seed text:In short, neural search is a new approach to retrieving information.GPT-2 can create amazing results (check out the English speaking unicorns example) but it takes its sweet time and gobbles up CPU and memory. Here’s what it spat out after about 10 minutes:In short, neural search is a new approach to retrieving information. In an ideal world, there would be lots of new data to search, but most of it is already stored in a relational database. That’s why most of the time it’s best not to store any information in the database at all. And in the future, what we want is that the information is simply searchable. But even if the information is readily searchable, there is still the problem of how to combine multiple search terms in the database. One way to combine search terms is to have search engines, but when we search for something on Google or any other search engine, they can often be quite good, but sometimes they’ll make mistakes, and they’ll only show some of the results, and sometimes they won’t show all of the results or they’ll say something very specific but you can’t look it up in that particular search. One of the most important goals of neural search is to let them search even more easily so they can make better decisions. But there’s still one issue: some types of problems don’t lend themselves to it and this can lead to a lot of lost information if the search engine doesn’t have the right tool in the right place. Nurture Now, I think we can take some lessons from neural search to solve this problem...Looks somewhat coherent to me, though it doesn’t keep a clear narrative going, and Dear Lord, some of those run-on sentences!distilGPT2 is a more efficient version of GPT-2 that had some excess neurons pruned away. Judging by its outputs, maybe a few too many neurons! It took about a minute to generate each result:In short, neural search is a new approach to retrieving information. But in recent years, the field has gained little traction, and now there is evidence that it is more accurate in detecting and analyzing neural networks on human skin. Here is the gist of this story. The researchers of the Chinese National Scientific Research Foundation (CSRF) have teamed up with researchers from the University of California, San Diego, who are working on a neuroimaging and learning network that will map up and reconstruct neural networks on human skin. This brain project aims to look into the brain’s visual cortex, so that it can learn from individuals without losing information. The CSRF will also use a neural network to map the brain’s sensory cortex and the cerebral cortex, as well as the anterior and caudate cortex, which is part of the brain’s central cortex. But this may not be the first step in the quest to decipher neural networks, a method used to search for patterns of information on a wide variety of people, including those who are not on the receiving end of a drug. Researchers at University of California, San Diego (USDA) in collaboration with scientists at the Chinese National Scientific Research Foundation (CSRF) in collaboration with CSRF, have developed a technique that allows the scientists to search for patterns of information on skin. The researchers analyzed the brain’s sensory...What if we throw that seed text at something meant to generate something different, like movie scripts?gpt2_film_scripts was made by fine-tuning GPT-2 on 1,300 screenplays, complete with the weird spacing and margins they have compared to regular text. This really shows when you generate examples. Like GPT-2 (which this model is based on), examples took about 10 minutes each:In short, neural search is a new approach to retrieving information. There have been many, many studies demonstrating that what we find, we search for. This is a kind of computer-aided search.ALICE I don’t like that. I feel like an a**h*le.She is still getting out of bed, having gotten the morning’s lesson. She sits at the desk, studying the computer screen. It is a huge pile of documents, and she quickly glances through them, and quickly notices the picture of a cat. She stares at the picture, and takes out a tiny file.As we can see, the language model has picked up how to swear and isn’t afraid to do it! (I had to manually censor its potty mouth). We can also see it understands character lines and stage directions. And just like every netizen out there, it has a thing for cats. (If it seems a little short compared to other samples, it’s because it adds a lot of spacing (for magins) that Medium strips out.One of my dreams is to create Star Trek episodes fully generated by an AI. Lieutenant Commander Data Science, if you will. So I trained gpt2_film_scripts on every Star Trek script I could find.And…after making that tragic Star Trek pun, my easy_text_generator crashed on me. I have a feeling these AI’s may be smarter (and more emotional) than we think.After restarting:In short, neural search is a new approach to retrieving information. A new way forward.FARRIS: The key is the neural patterns themselves, not the computer.SISKO: But a computer can do this.FARRIS: It’s a breakthrough in computational neural net theory. It is clear they need to be designed to learn, replicate, and then share their knowledge for practical applications.SISKO: Then you’ve got a problem. How do we know it’ll work?FARRIS: We’ll probably never see it on paper, but the neural pattern generator is the single most promising piece of hardware I’ve ever seen written about, all things considered.SISKO: We’ll get there eventually. When we do, can we trust this?[Farris’ office]FARRIS: A neural pattern generator will be the single most promising piece of hardware I’ve ever seen written about all things considered.[Captain’s office](Morn is giving a speech at Starfleet Headquarters, reading.)FARRIS: My colleagues and I have spent a year developing the prototype, which will be ready for launch by the end of the year.SISKO: How will that affect Starfleet operations?Not bad at all. For anyone who knows Star Trek, that sounds mostly convincing (though Morn giving a speech is plainly hilarious.)As we can see above, GPT-2 is pretty powerful. both for generating general-purpose text and for more specific use cases (like Star Trek scripts).For these specific use cases though, GPT-2 needs a lot of data thrown at it, and a lot of epochs going over all that data. GPT-3 promises “few-shot learning”, so we could throw just a few examples at it and it would quickly pick up the style. So if you fancy a new book in the Lord of the Rings series, or a press release about <insert product here>, GPT-3 might be the language model for you!If you want an easy way to get started generating your own language, check out the easy_text_generator repo. I’d love to see what comes out!Alex is an open-source evangelist at Jina AI, builder of animatronic butterflies, and trainer of janky language models that write bad Star Trek scripts.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJul 19, 2020Member-onlyWe have recently heard that GPT-3, a Natural Language Processing(NLP) models have been made available by Open AI and is touted to be an invention bigger than blockchain. The extent of this transformation is such that this deep learning model can help things shown as sci-fi to become accessible to be implemented by college students.ERROR: Could not find article textJul 20, 2020Member-only“What’s the next big trend in programming? Maybe it’s sloppy programming” — Guy Steele on the Future of Programming LanguagesI want to discuss with you today four different kinds of programming. This framework should help you better understand the uniqueness of Deep Learning as a new way of programming. I characterize Deep…Jul 22, 2020Member-onlySaveOpenAI’s GPT-3 is the world’s most sophisticated natural language technology. It’s the latest and greatest text-generating neural network. And it has the Twittersphere abuzz.I want to speak about the implications of the latest hype. But first, a short description of the beast itself.While the vast majority of AI systems are designed for one use-case and tightly trained for that, GPT-3 is the latest in a lineage of natural language processing technologies know as Transformers that are far more generalizable.In effect, Transformers process massive amounts of text to learn some general attributes of language (pre-training). This knowledge is then used as a good starting point for finessing specific language understanding tasks (fine-tuning). In doing so, it dramatically reduces the amount of labelled training data required for specific natural language tasks.So what’s new? The main difference between GPT-3 and its predecessor, GPT-2, is its size.Size matters.In AI, size especially matters. Peter Norvig continues to be correct, only more so — “Quantity has a quality all of its own”. Specifically, it’s bigness (175B parameters) is directly related to its effectiveness (mean human accuracy of detecting if an article was AI-generated was ~52%).More concretely:Two days ago, Twitter lit up with interesting and excellent demos and projects built on top of GPT-3. Here are a few that stood out, and should give you a good flavour of what’s possible.Sushant Kumar has created this micro-site that takes a word and outputs a GPT-3 generated sentence based on that. Replace ‘word’ at the end of the following web address to see GPT-3 strut its stuff: https://thoughts.sushant-kumar.com/wordHere are some examples, only slightly cherry-picked.Sharif Shameem at debuild.co has built a few really exciting demos. Just describe what your app should do in plain English, then start using it within seconds.He has whipped up demos for:Kevin Lacker has sat GPT-3 down and given it the Turing Test. Kevin tests for Common Sense, Trivia and Logic. His conclusion: “GPT-3 is quite impressive in some areas, and still clearly subhuman in others.” A bit like myself.Traditionally, artificial intelligence struggles at “common sense”. Here’s GPT-3 answering a lot of common sense questions.Mckay Wrigley designed an app that uses GPT-3 to allow you to ‘learn from anyone’. Ever wanted to learn about rockets from Elon Musk? How to write better from Shakespeare? Philosophy from Aristotle?At the time of posting, learnfromanyone was down but should shortly be back up and running.Jordan Singer has built a plugin for Figma that uses GPT-3 to design with/for you. The plugin talks to Figma to say things like “add a blue square” or “give me a pink circle that’s 500px”.Jordan made a basic representation of the Figma canvas which GPT-3 generates after providing a few raw text examples. That representation is then translated into Figma plugin code to build a screen.Chris Lu built an Explain Like I’m 5 website using GPT-3.Paul Yacoubian uses GPT-3 seed primer to generate infinite ideas.GPT-3 is an inflection point for natural language processing. But not because it’s a great conceptual leap forward.GPT-3 feels different. The range of demos attest to that. It has poured burning fuel on a flammable hype factory. A factory that’s almost religiously primed to latch on to any promise of computers appearing to act intelligently.Why?For a start, it’s better than the previous generation of models. It’s passed some invisible threshold to being obviously fun and useful for a newbie from GPT-2 which required more time invested by the user (e.g. Gwern).It’s also highly accessible. Opening access via the beta API has led to a furious spread of demos and projects — and this brings the potential of the technology to life. Tangibility is important.On top of that, you can interact with GPT-3 via English. It requires only a minimal amount of specific knowledge. The profiles of those creating the demos and cheering them on from the sidelines is different than before. It’s not just AI practitioners and trend-watchers. It’s hackers.My prediction is that the primary legacy of GPT-3 will be a dramatic expansion of who will try new things with AI and what they’ll be inspired to try with it.This may well be the Homebrew Computer Club for the newest generation of NLP.So what next?I agree with Azeem Azhar when he draws a parallel to how the boom in machine vision capabilities triggered a wave of AI startups, VC investment, and corporate spending. This is what pure-play AI VC funding looked like at the start of the computer vision boom.Azeem puts it nicely:And so today in 2020, we think nothing of high-quality computational manipulation of images. We open our phones with our faces. We track visitors with our cheap consumer webcams. I have a free app that recognises any plant I point my phone at. Capabilities that were not available anywhere in the world a decade ago are now too cheap to meter. Autonomous vehicles are dependent on these breakthroughs to navigate the roads.If GPT-3 and natural language processing follows a similar path to machine vision, we can expect rapid improvements in applications that use text in the coming 3–5 years.This matters.What would a flurry in investment in NLP mean?For a start, we can expect a proliferation of standard NLP use-cases, driven by performance improvement and increased accessibility.These NLP tasks are not new; they’ve been around since the ’80s. Most of us use things like semantic search, machine translation and chatbots on a regular or semi-regular basis. Sizeable improvement would mean greater adoption and potentially lead to a change in who provides these services. For example, could dramatically improved semantic search built on a pay-per access AI layer loosen Google’s grip on Search?There’s also the downstream implications for the use of Voice. While usable, Voice today remains clunky. Filling in the many gaps in Siri’s and Alexa’s knowledge would go a long way to instilling confidence in the user, and spark increased usage of voice in the first place.Then there’s the impact on content generation. Who’d bother writing when an AI can do it for you? Well, me for a start. Plus a myriad of others far more talented. Where AI content generation has real potential to disrupt is at the lowest end. The real losers here will be long-tail, low-end content generators — the WikiHows and below.The greatest potential as I see it, however, sits outside these well-trodden, general paths. Relaxing the need for massive amounts of training data shifts what use-cases NLP can be pointed toward. Bringing NLP to neglected, unattractive corners of commerce would be a game-changer for areas such as financial reporting, technical writing, and medical reporting. Augmenting the capabilities of readers and writers shifts how this type of content is created and consumed … and by whom.Finally, there’s the second order implications of mass generation of AI text. Deep-fakes aren’t going away anytime soon and so authentication of content just became more important.This was originally published on simonoregan.com.If you enjoyed this, you might like The Deployment Age — a weekly update of tools and musings that shine some light on the emerging technologies and trends of the 2020s.ERROR: Could not find article textJun 17, 2020Member-onlySaveGoogle Cloud Platform’s BigQuery is able to ingest multiple file types into tables. In this tutorial, I’ll show what kind of files it can process and why you should use Parquet whenever possible.For full access to all Medium articles — including mine — consider subscribing here.To start us off we’ll need a dataset in which the tables will live:The above snippet will create a dataset called files. Here’s an explanation of the parameters used:I’ll run the code from cloud shell as it is most convenient — I highly recommend you try it from there too.Once it runs, we can confirm from the UI that all settings are set properly:Now that we have a dataset to populate, let’s use Python (sklearn in particular) to create some data for us and save it as a comma-separated file.This is a very simple dataset so we can actually use BigQuery’s autodetect feature to figure out the schema for it. Here’s how to do this from the command line using bq:If for some reason this can’t find the files dataset make you can specify your project name as part of the table name or you can set the default project to the project you used before:gcloud config set core/project YOUR_PROJECT_NAMELet’s have a look at doing this via the gcloud SDK for Python:Note that here, we uploaded the file directly from local storage to BigQuery, but it is recommended to first copy your files to GCS (Google Cloud Storage) and then load the files from there. Check out load_table_from_uri on how to do this.When it comes to file types we do have a few options support by BigQuery:But they are not all created equal…Notice that in the above file everything is numerical and that makes detecting column types easy for BigQuery. But when you start loading in strings and dates and times and floats and ints and mixed fields then life becomes complicated and the schema autodetect feature of BigQuery gives up. Then you will need to specify the schema yourself and this can get tedious and messy very quickly as there is no 1-to-1 mapping of Numpy datatypes to BigQuery. In this case, Avro and Parquet formats are a lot more useful. They store metadata about columns and BigQuery can use this info to determine the column types!Avro is the recommended file type for BigQuery because its compression format allows for quick parallel uploads but support for Avro in Python is somewhat limited so I prefer to use Parquet.To demonstrate where autodetect would fail let’s add a few more rows and a funky column that has missing values everywhere except for 2 floats in the very end.This works without errors. But it did it do what we wanted it to do? Let’s check the data types:This gives us the following:Our beloved floats became strings!CSV format is messy. There’s no clear guidance on how you should quote strings, what delimiter you should use, whether there’s a header and then we didn’t even touch on datasets spread across multiple files.Parquet files, on the other hand, carry metadata about the columns. They know what type each column should be so when you load it into BigQuery, there’s no more guessing involved. Even better, parquet files are usually compressed column-wise so your categorical columns will become a lot cheaper to store! Try it out!Let’s see how we can upload the same dataframeto BigQuery as a parquet file:This is simpler — notice that we no longer need to skip the headers or guess a schema — and best of all it comes with compression. Our file size went from 1.8MB to 59KB (we have to note that this is mostly because of the many duplicated values, but still very impressive)!And our column types are now correct:We showed how BigQuery can ingest CSV and Parquet files and how CSV can lead to complications. If you think the above example won’t happen in real life, think again! When you have 1000s of columns spread across many 1000s of large files the likelihood of BigQuery detecting wrong column types is quite high — also the 1000s of columns won’t help when you need to write the schema for it…So the next time you’re expecting a large file that you need to load into BigQuery ask for Parquet!For more fun with BigQuery check out this article where I show how you can find Fibonacci numbers using UDFs.All the code used above can be found on GitHub.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJun 2, 2020Member-onlyI was showing how to create a basic Room database in my previous post. This time I’m taking it further by introducing an Export to CSV file option to the…ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article text