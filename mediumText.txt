Jun 16, 2020Member-onlyIs it a Memory Bank or an NLP Ultimatum?So last week OpenAI told the world about this gigantic model which was trained on 174,600 million parameters and released a 74-page research paper titled Language Models are Few-Shot Learners.If you donâ€™t get the depth of this number then see the parameters used in the past models.ERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textERROR: Could not find article textJun 19, 2020Member-onlyOpenAI recently published a paper describing GPT-3, a deep-learning model for Natural Language Processing, with 175 Billion parameters(!!!), 100x more than the previous version, GPT-2. The model is pre-trained on nearly half a trillion words and achieves SOTA performance on several NLP benchmarks without fine-tuning.ERROR: Could not find article text